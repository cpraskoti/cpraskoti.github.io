---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

<style>
.publication-item {
    margin-bottom: 2em;
}
.publication-title {
    font-size: 1.1em;
    font-weight: 600;
    margin-bottom: 0.3em;
}
.publication-venue {
    color: #666;
    font-style: italic;
    margin-bottom: 0.3em;
}
.publication-description {
    line-height: 1.5;
}
</style>

<div class="publication-item">
    <div class="publication-title">
        <a href="https://arxiv.org/abs/2504.05059" target="_blank">MIAT: Maneuver-Intention-Aware Transformer for Spatio-Temporal Trajectory Prediction</a>
    </div>
    <div class="publication-venue">
        IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2025
    </div>
    <div class="publication-description">
        This paper presents a Maneuver Intention-Awareness control mechanism for vehicle trajectory prediction in mixed traffic.
    </div>
</div>

<div class="publication-item">
    <div class="publication-title">
        Elevation-aware 2D/3D co-simulation framework for large-scale traffic flow and high-fidelity vehicle dynamics
    </div>
    <div class="publication-venue">
        Preprint (Draft), 2025
    </div>
    <div class="publication-description">
        A 2D/3D traffic co-simulation framework that generates elevation-aware 3D road networks from real-world GIS data to support perception, simulation, and evaluation of intelligent transportation ML models and autonomous driving systems.
    </div>
</div>

<div class="publication-item">
    <div class="publication-title">
        Continual Learning With Hard Attention Parameter Masking on Image Classification Tasks
    </div>
    <div class="publication-venue">
        IOE Graduate Conference (IOEGC), 2024
    </div>
    <div class="publication-description">
        This paper presents a continual learning method that applies hard-attention parameter masking to mitigate catastrophic forgetting in image classification, allowing neural networks to acquire new tasks over time while preserving prior knowledge.
    </div>
</div>
